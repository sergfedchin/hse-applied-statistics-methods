{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, sys\n",
    "practice_dir = os.path.join(os.path.dirname(os.path.abspath('')), 'practice')\n",
    "if not practice_dir in sys.path:\n",
    "    sys.path.append(practice_dir)\n",
    "from utils import isnumber, type_name, raise_operand_exception, FwdAAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BwdAAD:\n",
    "    def __init__(self, value: int | float = None, local_gradients: defaultdict = None, symbol: str = None):\n",
    "        self.value: float | int = value\n",
    "        self.symbol: str = symbol\n",
    "        self.local_gradients: defaultdict = local_gradients if local_gradients else defaultdict(float, {self: 1})\n",
    "    \n",
    "    def get_variable(symbol: str, value: int | float = None):\n",
    "        \"\"\"\n",
    "        Create variable symbol.\n",
    "\n",
    "        :param symbol: unique name to indicate the variable\n",
    "        :param value: value of the variable\n",
    "        :returns: BwdAAD  variable\n",
    "        `ignore_existent` is False\n",
    "        \"\"\"\n",
    "        return BwdAAD(value=value, symbol=symbol)\n",
    "    \n",
    "    def get_vector(symbol: str, values: Iterable[int | float] = None, length: int = None):\n",
    "        \"\"\"\n",
    "        Create a list of variable symbols.\n",
    "\n",
    "        :param symbol: unique name to indicate the variables. Each variable will have a name in format 'symbol'_i\n",
    "        :param values: values of the variables\n",
    "        :param length: length of the vector (is required if `values = None`, otherwise ignored)\n",
    "        :returns: BwdAAD  variables list\n",
    "        :raises ValueError: if neither `values` nor `length` arguments have been specified\n",
    "        \"\"\"\n",
    "        if values:\n",
    "            return [BwdAAD.get_variable(symbol + '_' + str(i + 1), value=val) for i, val in enumerate(values)]\n",
    "        elif length:\n",
    "            return [BwdAAD.get_variable(symbol=symbol + '_' + str(i + 1)) for i in range(length)]\n",
    "        else:\n",
    "            raise ValueError(\"Please provide values or specify length of the vector.\")\n",
    "\n",
    "    def set_value(self, value: float | int):\n",
    "        \"\"\"\n",
    "        Change value of the variable to `value`. Note that this deletes the\n",
    "        computed gradient for the variable.\n",
    "\n",
    "        :param value: new value of the variable\n",
    "        :raises ValueError: if `value` is not a number\n",
    "        \"\"\"\n",
    "        if not isnumber(value):\n",
    "            raise ValueError(f\"Expected a number, got '{type_name(value)}'\")\n",
    "        self.value = value\n",
    "        self.local_gradients = defaultdict(float, {self: 1})\n",
    "\n",
    "    def set_name(self, name: str):\n",
    "        \"\"\"\n",
    "        Change the symbol of the variable to `name`. Note that this deletes the\n",
    "        computed gradient for the variable.\n",
    "\n",
    "        :param name: new name of the variable\n",
    "        :raises ValueError: if `name` is not a str\n",
    "        \"\"\"\n",
    "        if not isinstance(name, str):\n",
    "            raise ValueError(f\"Expected a 'str', got '{type_name(value)}'\")\n",
    "        self.symbol = name\n",
    "        self.local_gradients = defaultdict(float, {self: 1})\n",
    "\n",
    "    def set_vector_values(vector: Iterable, values: Iterable[int | float]) -> None:\n",
    "        \"\"\"\n",
    "        Change values of an entire vector of variable to `values`. Note that this deletes the\n",
    "        computed gradient for the variables.\n",
    "\n",
    "        :param vector: an iterable of variables to change values of\n",
    "        :param values: new values of the variables\n",
    "        :raises ValueError: if not a number is found in the `values`\n",
    "        :raises ValueError: lengths of `vector` and `values` mismatch\n",
    "        \"\"\"\n",
    "        if len(values) != len(vector):\n",
    "            raise ValueError(\"`len(values)` must be the same as `len(vector)`\")\n",
    "        for var, val in zip(vector, values):\n",
    "            var.set_value(val)\n",
    "\n",
    "    def get_gradient(self) -> dict:\n",
    "        \"\"\"\n",
    "        Compute the first derivatives of the variable with respect to all its child variables.\n",
    "\n",
    "        :returns: dict which maps child variable to the derivative of the function w.r.t. this variable\n",
    "        \"\"\"\n",
    "        gradients = defaultdict(float)\n",
    "        def compute_gradients(variable: BwdAAD, path_value):\n",
    "            for child_variable, local_gradient in variable.local_gradients.items():\n",
    "                # multiply the edges of a path\n",
    "                path_to_child_value = path_value * local_gradient\n",
    "                # add together different paths\n",
    "                gradients[child_variable] += path_to_child_value\n",
    "                # recurse through graph if it is not the user-initialised variable\n",
    "                if not child_variable.symbol:\n",
    "                    compute_gradients(child_variable, path_to_child_value)\n",
    "        compute_gradients(self, path_value=1) # path_value=1 is from `variable` differentiated w.r.t. itself\n",
    "        return dict(gradients)\n",
    "    \n",
    "    def print_gradient(self, precision: int = 3):\n",
    "        print(', '.join(['d/d{} = {:.{}f}'.format(key.symbol, d, precision) for key, d in sorted(list(self.get_gradient().items()), key=lambda x: x[0].symbol if x[0].symbol else '') if key.symbol]))\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, BwdAAD):\n",
    "            value = self.value + other.value\n",
    "            local_gradients = defaultdict(float)\n",
    "            local_gradients[self] += 1\n",
    "            local_gradients[other] += 1\n",
    "            return BwdAAD(value, local_gradients)\n",
    "        if isnumber(other):\n",
    "            return BwdAAD(self.value + other, defaultdict(float, {self: 1}))\n",
    "        raise_operand_exception(self, other, '+')\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, BwdAAD):\n",
    "            value = self.value * other.value\n",
    "            local_gradients = defaultdict(float)\n",
    "            local_gradients[self] += other.value\n",
    "            local_gradients[other] += self.value\n",
    "            res = BwdAAD(value, local_gradients)\n",
    "            return res\n",
    "        if isnumber(other):\n",
    "            res = BwdAAD(self.value * other, defaultdict(float, {self: other}))\n",
    "            return res\n",
    "        raise_operand_exception(self, other, '*')\n",
    "    \n",
    "    def __neg__(self):\n",
    "        value = -1 * self.value\n",
    "        local_gradients = defaultdict(float, {self: -1})\n",
    "        return BwdAAD(value, local_gradients)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def _inverse(self):\n",
    "        value = 1 / self.value\n",
    "        local_gradients = defaultdict(float, {self: -1 / self.value ** 2})\n",
    "        return BwdAAD(value, local_gradients)   \n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other._inverse()\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return self._inverse() * other\n",
    "\n",
    "    __rmul__ = __mul__\n",
    "    __radd__ = __add__\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        if isinstance(other, BwdAAD):\n",
    "            value = pow(self.value, other.value)\n",
    "            local_gradients = defaultdict(float)\n",
    "            local_gradients[self] += other.value * pow(self.value, other.value - 1)\n",
    "            local_gradients[other] += pow(self.value, other.value) * np.log(self.value)\n",
    "            return BwdAAD(value, local_gradients)\n",
    "        if isnumber(other):\n",
    "            value = pow(self.value, other)\n",
    "            local_gradients = defaultdict(float, {self: other * pow(self.value, other - 1)})\n",
    "            return BwdAAD(value, local_gradients)\n",
    "        raise_operand_exception(self, other, '** or pow()')\n",
    "\n",
    "    def __rpow__(self, other):\n",
    "        if isnumber(other):\n",
    "            value = pow(other, self.value)\n",
    "            local_gradients = defaultdict(float, {self: pow(other, self.value) * np.log(other)})\n",
    "            return BwdAAD(value, local_gradients)\n",
    "        raise_operand_exception(self, other, '** or pow()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользоваться им нужно так (я постарался сохранить максимальную совместимость с классом `FwdAAD` с практики, поэтому код в точности такой же за исключением отсутствие флага `ignore_existent`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d/dx = -66.9568352087, d/dy = -20.6566692542\n",
      "d/dy_0 = -1, d/dy_1 = -1\n"
     ]
    }
   ],
   "source": [
    "x: BwdAAD = BwdAAD.get_variable('x', 2)\n",
    "y: BwdAAD = BwdAAD.get_variable('y', np.pi)\n",
    "\n",
    "def func(x, y):\n",
    "    return x ** 3 - 2 * x ** 2 * y ** 2 + y ** 3\n",
    "\n",
    "f: BwdAAD = func(x, y)\n",
    "f.print_gradient(precision=10)\n",
    "\n",
    "# if we want to change values and names:\n",
    "x.set_value(1)\n",
    "y.set_value(1)\n",
    "x.set_name('y_0')\n",
    "y.set_name('y_1')\n",
    "\n",
    "f: BwdAAD = func(x, y)\n",
    "f.print_gradient(precision=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сравним производительность этого варианта вычисления градиента с forward-mode методом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_1(x):\n",
    "    return np.sum(np.power(x, 2))\n",
    "def f_2(x):\n",
    "    return np.sum(np.power(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VARIABLES = 11\n",
    "NUM_ARGUMENTS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1771561/1771561 [02:52<00:00, 10283.69it/s]\n",
      "100%|██████████| 1771561/1771561 [04:48<00:00, 6132.65it/s]\n"
     ]
    }
   ],
   "source": [
    "x_fwd = FwdAAD.get_vector('X', length=NUM_VARIABLES)\n",
    "\n",
    "FwdAAD.set_vector_values(x_fwd, np.linspace(0, 1, NUM_VARIABLES))\n",
    "\n",
    "start_time_fwd_1 = time.time()\n",
    "for arguments in tqdm(itertools.product(x_fwd, repeat=NUM_ARGUMENTS), total=NUM_VARIABLES ** NUM_ARGUMENTS):\n",
    "    f_1(arguments).get_gradient()\n",
    "time_fwd_1 = int(time.time() - start_time_fwd_1)\n",
    "\n",
    "start_time_fwd_2 = time.time()\n",
    "FwdAAD.set_vector_values(x_fwd, np.linspace(1, 2, NUM_VARIABLES))\n",
    "for arguments in tqdm(itertools.product(x_fwd, repeat=NUM_ARGUMENTS), total=NUM_VARIABLES ** NUM_ARGUMENTS):\n",
    "    f_2(arguments).get_gradient()\n",
    "time_fwd_2 = int(time.time() - start_time_fwd_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1771561/1771561 [01:06<00:00, 26782.71it/s]\n",
      "100%|██████████| 1771561/1771561 [01:25<00:00, 20632.29it/s]\n"
     ]
    }
   ],
   "source": [
    "x_bwd = BwdAAD.get_vector('X', length=NUM_VARIABLES)\n",
    "\n",
    "BwdAAD.set_vector_values(x_bwd, np.linspace(0, 1, NUM_VARIABLES))\n",
    "\n",
    "start_time_bwd_1 = time.time()\n",
    "for arguments in tqdm(itertools.product(x_bwd, repeat=NUM_ARGUMENTS), total=NUM_VARIABLES ** NUM_ARGUMENTS):\n",
    "    f_1(arguments).get_gradient()\n",
    "time_bwd_1 = int(time.time() - start_time_bwd_1)\n",
    "\n",
    "start_time_bwd_2 = time.time()\n",
    "BwdAAD.set_vector_values(x_bwd, np.linspace(1, 2, NUM_VARIABLES))\n",
    "for arguments in tqdm(itertools.product(x_bwd, repeat=NUM_ARGUMENTS), total=NUM_VARIABLES ** NUM_ARGUMENTS):\n",
    "    f_2(arguments).get_gradient()\n",
    "time_bwd_2 = int(time.time() - start_time_bwd_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward-mode autodiff:  2m 52s and 4m 48s\n",
      "Backward-mode autodiff: 1m 6s and 1m 25s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forward-mode autodiff:  {time_fwd_1 // 60}m {time_fwd_1 % 60}s and {time_fwd_2 // 60}m {time_fwd_2 % 60}s\")\n",
    "print(f\"Backward-mode autodiff: {time_bwd_1 // 60}m {time_bwd_1 % 60}s and {time_bwd_2 // 60}m {time_bwd_2 % 60}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward-mode оказался значительно эффективнее forward-mode!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
